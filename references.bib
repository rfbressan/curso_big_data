@article{Fellner2013,
  title={Testing enforcement strategies in the field: Threat, moral appeal and social information},
  author={Fellner, Gerlinde and Sausgruber, Rupert and Traxler, Christian},
  journal={Journal of the European Economic Association},
  volume={11},
  number={3},
  pages={634--660},
  year={2013},
  publisher={Oxford University Press}
}

@article{Ferman2019,
  title={A simple way to assess inference methods},
  author={Ferman, Bruno},
  journal={arXiv preprint arXiv:1912.08772},
  year={2019}
}

@article{Dwenger2016,
  title={Extrinsic and intrinsic motivations for tax compliance: Evidence from a field experiment in Germany},
  author={Dwenger, Nadja and Kleven, Henrik and Rasul, Imran and Rincke, Johannes},
  journal={American Economic Journal: Economic Policy},
  volume={8},
  number={3},
  pages={203--32},
  year={2016}
}

@article{Luttmer2014,
  title={Tax morale},
  author={Luttmer, Erzo FP and Singhal, Monica},
  journal={Journal of economic perspectives},
  volume={28},
  number={4},
  pages={149--68},
  year={2014}
}

@article{Naritomi2019,
  title={Consumers as tax auditors},
  author={Naritomi, Joana},
  journal={American Economic Review},
  volume={109},
  number={9},
  pages={3031--72},
  year={2019}
}

@article{Duflo2007,
  title={Using randomization in development economics research: A toolkit},
  author={Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  journal={Handbook of development economics},
  volume={4},
  pages={3895--3962},
  year={2007},
  publisher={Elsevier}
}

@article{Ai2003,
  title={Interaction terms in logit and probit models},
  author={Ai, Chunrong and Norton, Edward C},
  journal={Economics letters},
  volume={80},
  number={1},
  pages={123--129},
  year={2003},
  publisher={Elsevier}
}

@book{Imbens2015,
  title={Causal inference in statistics, social, and biomedical sciences},
  author={Imbens, Guido W and Rubin, Donald B},
  year={2015},
  publisher={Cambridge University Press}
}

@article{Bowles2001,
Author = {Bowles, Samuel and Gintis, Herbert and Osborne, Melissa},
Title = {The Determinants of Earnings: A Behavioral Approach},
Journal = {Journal of Economic Literature},
Volume = {39},
Number = {4},
Year = {2001},
Month = {December},
Pages = {1137-1176},
DOI = {10.1257/jel.39.4.1137},
URL = {https://www.aeaweb.org/articles?id=10.1257/jel.39.4.1137}}

@book{Ashenfelter2010,
  title={Handbook of labor economics},
  author={Ashenfelter, Orley and Card, David},
  year={2010},
  publisher={Elsevier}
}

@article{Imbens2015b,
  title={Matching methods in practice: Three examples},
  author={Imbens, Guido W},
  journal={Journal of Human Resources},
  volume={50},
  number={2},
  pages={373--419},
  year={2015},
  publisher={University of Wisconsin Press}
}

@book{Angrist2008,
  title={Mostly harmless econometrics: An empiricist's companion},
  author={Angrist, Joshua D and Pischke, J{\"o}rn-Steffen},
  year={2008},
  publisher={Princeton university press}
}

@article{Belloni2014,
  title={Inference on treatment effects after selection among high-dimensional controls},
  author={Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
  journal={The Review of Economic Studies},
  volume={81},
  number={2},
  pages={608--650},
  year={2014},
  publisher={Oxford University Press}
}

@article{Wager2018,
  title={Estimation and inference of heterogeneous treatment effects using random forests},
  author={Wager, Stefan and Athey, Susan},
  journal={Journal of the American Statistical Association},
  volume={113},
  number={523},
  pages={1228--1242},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{Athey2019,
  title={Estimating treatment effects with causal forests: An application},
  author={Athey, Susan and Wager, Stefan},
  journal={arXiv preprint arXiv:1902.07409},
  year={2019}
}

@article{Breiman2001,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{Athey2019b,
  title={Generalized random forests},
  author={Athey, Susan and Tibshirani, Julie and Wager, Stefan and others},
  journal={The Annals of Statistics},
  volume={47},
  number={2},
  pages={1148--1178},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@article{Chernozhukov2018a,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = "{Double/debiased machine learning for treatment and structural parameters}",
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@misc{singh2020debiased,
      title={De-biased Machine Learning in Instrumental Variable Models for Treatment Effects}, 
      author={Rahul Singh and Liyang Sun},
      year={2020},
      eprint={1909.05244},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Rubin1974,
  title={Estimating causal effects of treatments in randomized and nonrandomized studies.},
  author={Rubin, Donald B},
  journal={Journal of educational Psychology},
  volume={66},
  number={5},
  pages={688},
  year={1974},
  publisher={American Psychological Association}
}

@article{davis2017using,
  title={Using causal forests to predict treatment heterogeneity: An application to summer jobs},
  author={Davis, Jonathan and Heller, Sara B},
  journal={American Economic Review},
  volume={107},
  number={5},
  pages={546--50},
  year={2017}
}

@article{suk2020random,
  title={Random forests approach for causal inference with clustered observational data},
  author={Suk, Youmi and Kang, Hyunseung and Kim, Jee-Seon},
  journal={Multivariate Behavioral Research},
  pages={1--24},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{athey2019estimating,
  title={Estimating treatment effects with causal forests: An application},
  author={Athey, Susan and Wager, Stefan},
  journal={arXiv preprint arXiv:1902.07409},
  year={2019}
}

@article{wager2018estimation,
  title={Estimation and inference of heterogeneous treatment effects using random forests},
  author={Wager, Stefan and Athey, Susan},
  journal={Journal of the American Statistical Association},
  volume={113},
  number={523},
  pages={1228--1242},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{lechner2018modified,
  title={Modified causal forests for estimating heterogeneous causal effects},
  author={Lechner, Michael},
  journal={arXiv preprint arXiv:1812.09487},
  year={2018}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{athey2019generalized,
  title={Generalized random forests},
  author={Athey, Susan and Tibshirani, Julie and Wager, Stefan and others},
  journal={The Annals of Statistics},
  volume={47},
  number={2},
  pages={1148--1178},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{oprescu2019orthogonal,
  title={Orthogonal random forest for causal inference},
  author={Oprescu, Miruna and Syrgkanis, Vasilis and Wu, Zhiwei Steven},
  booktitle={International Conference on Machine Learning},
  pages={4932--4941},
  year={2019},
  organization={PMLR}
}

@article{Chernozhukov2018,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = "{Double/debiased machine learning for treatment and structural parameters}",
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@techreport{varian2018artificial,
  title={Artificial intelligence, economics, and industrial organization},
  author={Varian, Hal},
  year={2018},
  institution={National Bureau of Economic Research}
}

@article{athey2019machine,
  title={Machine learning methods that economists should know about},
  author={Athey, Susan and Imbens, Guido W},
  journal={Annual Review of Economics},
  volume={11},
  pages={685--725},
  year={2019},
  publisher={Annual Reviews}
}

@article{andini2018targeting,
  title={Targeting with machine learning: An application to a tax rebate program in Italy},
  author={Andini, Monica and Ciani, Emanuele and de Blasio, Guido and D'Ignazio, Alessio and Salvestrini, Viola},
  journal={Journal of Economic Behavior \& Organization},
  volume={156},
  pages={86--102},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{shao2002applying,
  title={Applying data mining to detect fraud behavior in customs declaration},
  author={Shao, Hua and Zhao, Hong and Chang, Gui-Ran},
  booktitle={Proceedings. International Conference on Machine Learning and Cybernetics},
  volume={3},
  pages={1241--1244},
  year={2002},
  organization={IEEE}
}

@article{sokolov2016economic,
  title={Economic growth forecasting by artificial neural network with extreme learning machine based on trade, import and export parameters},
  author={Sokolov-Mladenovi{\'c}, Svetlana and Milovan{\v{c}}evi{\'c}, Milos and Mladenovi{\'c}, Igor and Alizamir, Meysam},
  journal={Computers in Human Behavior},
  volume={65},
  pages={43--45},
  year={2016},
  publisher={Elsevier}
}

@techreport{goldfarb2018ai,
  title={AI and international trade},
  author={Goldfarb, Avi and Trefler, Daniel},
  year={2018},
  institution={National Bureau of Economic Research}
}

@inproceedings{de2018tax,
  title={Tax fraud detection for under-reporting declarations using an unsupervised machine learning approach},
  author={de Roux, Daniel and Perez, Boris and Moreno, Andr{\'e}s and Villamil, Maria del Pilar and Figueroa, C{\'e}sar},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={215--222},
  year={2018}
}

@article{gonzalez2013characterization,
  title={Characterization and detection of taxpayers with false invoices using data mining techniques},
  author={Gonz{\'a}lez, Pamela Castell{\'o}n and Vel{\'a}squez, Juan D},
  journal={Expert Systems with Applications},
  volume={40},
  number={5},
  pages={1427--1436},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{adamov2019machine,
  title={Machine Learning and Advanced Analytics in Tax Fraud Detection},
  author={Adamov, Abzetdin Z},
  booktitle={2019 IEEE 13th International Conference on Application of Information and Communication Technologies (AICT)},
  pages={1--5},
  year={2019},
  organization={IEEE}
}

@inproceedings{paula2016deep,
  title={Deep learning anomaly detection as support fraud investigation in brazilian exports and anti-money laundering},
  author={Paula, Ebberth L and Ladeira, Marcelo and Carvalho, Rommel N and Marzagao, Thiago},
  booktitle={2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  pages={954--960},
  year={2016},
  organization={IEEE}
}

@incollection{athey2018impact,
  title={The impact of machine learning on economics},
  author={Athey, Susan},
  booktitle={The economics of artificial intelligence: An agenda},
  pages={507--547},
  year={2018},
  publisher={University of Chicago Press}
}

@incollection{lederman2018comment,
  title={Comment on" The Impact of Machine Learning on Economics"},
  author={Lederman, Mara},
  booktitle={The Economics of Artificial Intelligence: An Agenda},
  pages={548--551},
  year={2018},
  publisher={University of Chicago Press}
}

@article{judge2019combining,
  title={Combining the Information From Econometrics Learning (EL) and Machine Learning (ML)},
  author={Judge, George},
  journal={Available at SSRN 3376953},
  year={2019}
}

@article{athey2017state,
  title={The state of applied econometrics: Causality and policy evaluation},
  author={Athey, Susan and Imbens, Guido W},
  journal={Journal of Economic Perspectives},
  volume={31},
  number={2},
  pages={3--32},
  year={2017}
}

@article{mullainathan2017machine,
  title={Machine learning: an applied econometric approach},
  author={Mullainathan, Sendhil and Spiess, Jann},
  journal={Journal of Economic Perspectives},
  volume={31},
  number={2},
  pages={87--106},
  year={2017}
}

